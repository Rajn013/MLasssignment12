#!/usr/bin/env python
# coding: utf-8

# ans 1 The number of desired outcomes is 3 (rolling a 2, 4, or 6), and there are 6 outcomes in total. The a priori probability for this example is calculated as follows: A priori probability = 3 / 6 = 50%. Therefore, the a priori probability of rolling a 2, 4, or 6 is 50%.
# 

# ans 2. Posterior probability is a revised probability that takes into account new available information. For example, let there be two urns, urn A having 5 black balls and 10 red balls and urn B having 10 black balls and 5 red balls. Now if an urn is selected at random, the probability that urn A is chosen is 0.5.
# 

# ans 3.Now suppose the same coin is tossed 50 times, and it shows heads only 14 times. You would assume that the likelihood of the unbiased coin is very low. If the coin were fair, it would have shown heads and tails the same number of times.
# 
# Example Scenario

# ans 4. Naïve Bayes classification is called Naïve because it assumes class conditional independence. The effect of an attribute value on a given class is independent of the values of the other attributes. This assumption is made to reduce computational costs and hence is considered Naïve.

# ans 5 .Bayes Optimal Classifier is a probabilistic framework that finds the most probable prediction using the training data and space of hypotheses to make a prediction for a new data instance.

# ans 6.prior knowledge is provided by asserting (1) a prior probability for each candidate hypothesis, and (2) a probability distribution over observed data for each possible hypothesis.

# ans 7 A learner L using a hypothesis H and training data D is said to be a consistent learner if it always outputs a hypothesis with zero error on D whenever H contains such a hypothesis.

# ans 8. t is simple and easy to implement.
# It doesn't require as much training data.
# It handles both continuous and discrete data.

# ans 9. .Lack of flexibility:Because Naive Bayes is a parametric model, it needs a set of predetermined parameters that must be learned from training data. Its ability to handle complicated and non-linear relationships between features may be constrained as a result.
# 
# 3.Data scarcity:For Naive Bayes to accurately estimate the conditional probabilities of each feature, there must be enough training data. Insufficient training data may cause the algorithm to underperform.

# ans 10. 1. Text classification is a machine learning technique that assigns a set of predefined categories to open-ended text. Text classifiers can be used to organize, structure, and categorize pretty much any kind of text – from documents, medical studies and files, and all over the web.
# 2.A spam filter is a program used to detect unsolicited, unwanted and virus-infected emails and prevent those messages from getting to a user's inbox. Like other types of filtering programs, a spam filter looks for specific criteria on which to base its judgments.
# 3.An investor can use market sentiment analysis to determine whether the market is driven by feelings and emotions or by rational decision-making. Market sentiment analysis is considered valuable as it can help you determine the opinion of investors.

# In[ ]:




